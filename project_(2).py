# -*- coding: utf-8 -*-
"""Project (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UU4IW1zo5wj4KsXLce_nKTw83-Z3YMUB

#    MACHINE LEARNING PROJECT

## **End-to-End Medical Triage Assistant**
### (UCI Heart Disease Dataset)

---

###   Project Description
This project focuses on building an **end-to-end machine learning–based medical triage system**  
that classifies patients into **Low, Medium, and High risk categories** using clinical data.  
The system incorporates preprocessing, class imbalance handling, probability calibration,  
model explainability, and ethical considerations to support clinical decision-making.

---

###  Student Information

- **Roll No:** 23-AI-25  
- **Roll No:** 23-AI-22  

---

###  Key Features
- Real-world medical dataset (UCI Heart Disease)
- Data preprocessing & SMOTE balancing
- Multiple ML models comparison
- Probability calibration
- Explainable AI (feature importance)
- User-input based patient risk prediction
- Ethical considerations in healthcare AI

#Imports Important Libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB

from sklearn.metrics import roc_auc_score, roc_curve
from sklearn.calibration import CalibratedClassifierCV

from imblearn.over_sampling import SMOTE

"""Explanation:

This block imports all required libraries for preprocessing, modeling, evaluation, and visualization.

#Dataset Loading
"""

data = pd.read_csv("heart_disease_uci.csv")
data.head()# View dataset structure

"""Explanation:

The dataset contains patient medical features information.

# Risk Level Creation using Binning
"""

# Create a new column 'risk' based on the values in 'num'
# pd.cut is used to divide numerical values into defined intervals (bins)

data["risk"] = pd.cut(
    data["num"],          # Numerical column used to define risk
    bins=[-1, 0, 2, 4],    # Range of values divided into bins
    labels=[0, 1, 2]      # Risk labels:
                          # 0 = Low Risk
                          # 1 = Medium Risk
                          # 2 = High Risk
).astype(int)             # Convert categorical output into integer type

# Check how many samples fall into each risk category
data["risk"].value_counts()

"""Explaination:

This code converts numerical values into discrete risk levels
(Low, Medium, High) using predefined intervals for classification.


"""

# ---- Innovation : Risk Score & Color Coding ----

def risk_score_and_color(prob):
    score = int(prob * 100)
    if score <= 30:
        return score, "Green (Low Risk)"
    elif score <= 60:
        return score, "Yellow (Moderate Risk)"
    else:
        return score, "Red (High Risk)"

"""#Feature–Target Separation and Column Type Identification"""

# Separate input features (X) and target variable (y)
X = data.drop(columns=["num", "risk"])   # Remove original numeric column and target
y = data["risk"]                         # Target variable (risk level)

# Identify categorical feature columns (text/object type)
categorical_cols = X.select_dtypes(include="object").columns.tolist()

# Identify numerical feature columns (int and float type)
numeric_cols = X.select_dtypes(exclude="object").columns.tolist()

# Display column groups
numeric_cols, categorical_cols

"""Explaination:

This code separates features and target, and identifies numerical
and categorical columns for appropriate data preprocessing.

#Data Preprocessing Pipeline (Numeric & Categorical Features)
"""

# Preprocessing steps for numerical features
numeric_transformer = Pipeline([
    ("imputer", SimpleImputer(strategy="median")),  # Fill missing numeric values with median
    ("scaler", StandardScaler())                    # Scale numerical features
])

# Preprocessing steps for categorical features
categorical_transformer = Pipeline([
    ("imputer", SimpleImputer(strategy="most_frequent")),  # Fill missing categorical values
    ("onehot", OneHotEncoder(handle_unknown="ignore"))     # Convert categories into numeric form
])

# Combine numerical and categorical preprocessing
preprocessor = ColumnTransformer([
    ("num", numeric_transformer, numeric_cols),
    ("cat", categorical_transformer, categorical_cols)
])

"""Explaination:

This preprocessing pipeline handles missing values and transforms
numerical and categorical features into a model-ready format.

#Split dataset into training and testing sets
"""

# Stratify is used to keep class distribution same in both sets

X_train, X_test, y_train, y_test = train_test_split(
    X,                   # Features
    y,                   # Target labels
    test_size=0.2,       # 20% data for testing, 80% for training
    stratify=y,          # Maintain same class balance in train and test sets
    random_state=42      # Ensure reproducibility
)

"""Explaination:

The data is split into training and testing sets while preserving
the class distribution to ensure fair model evaluation.

#Data Transformation and Class Balancing (SMOTE)
"""

# Apply preprocessing transformations to training and testing data
X_train_processed = preprocessor.fit_transform(X_train)  # Fit on training data
X_test_processed = preprocessor.transform(X_test)        # Transform test data

# Apply SMOTE to handle class imbalance in training data
smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(
    X_train_processed,   # Preprocessed training features
    y_train              # Target labels
)

# Compare class distribution before and after SMOTE
y_train.value_counts(), pd.Series(y_train_res).value_counts()

"""Explaination:

Preprocessing is applied to the data and SMOTE is used to balance
the training classes, improving model learning on minority risks.

#Train multiple models and calculate ROC-AUC
"""

# We train Logistic Regression, SVM, Random Forest, and Naive Bayes
# on the balanced training data and evaluate their performance using ROC-AUC.

models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),  # Logistic Regression model
    "SVM": SVC(probability=True),                                # SVM with probability output
    "Random Forest": RandomForestClassifier(n_estimators=300, random_state=42),  # Random Forest with 300 trees
    "Naive Bayes": GaussianNB()                                  # Gaussian Naive Bayes for continuous features
}

results = {}  # Dictionary to store ROC-AUC scores

for name, model in models.items():
    model.fit(X_train_res, y_train_res)           # Train the model on balanced training data
    y_prob = model.predict_proba(X_test_processed)  # Predict probabilities on test set

    auc = roc_auc_score(
        y_test,           # True labels
        y_prob,           # Predicted probabilities
        multi_class="ovr" # One-vs-Rest strategy for multi-class ROC-AUC
    )
    results[name] = auc  # Save ROC-AUC score

results  # Display ROC-AUC scores for all models

"""Explanation:

This code trains four different models on the balanced training data,
predicts probabilities on the test set, and calculates ROC-AUC scores.
The ROC-AUC score shows how well each model can distinguish between all classes.
Higher values mean better performance.

#Calibrated Random Forest Model
"""

# We first create a Random Forest classifier and then calibrate its probability outputs
# using isotonic regression to improve predicted probabilities.

# Create base Random Forest model
base_model = RandomForestClassifier(
    n_estimators=300,   # Number of trees in the forest
    random_state=42     # Ensures reproducibility
)

# Apply probability calibration
calibrated_model = CalibratedClassifierCV(
    base_model,         # Base model to calibrate
    method="isotonic"   # Isotonic regression for probability calibration
)

# Train the calibrated model on balanced training data
calibrated_model.fit(X_train_res, y_train_res)

# Predict probabilities on test data
y_calib_prob = calibrated_model.predict_proba(X_test_processed)

"""Explanation:

This code creates a Random Forest model and calibrates its predicted probabilities
using isotonic regression. Calibration improves the reliability of predicted probabilities,
making them more accurate for decision-making.

# Plot ROC curve for "High Risk" class
"""

# We calculate and plot the ROC curve for class 2 (High Risk) using calibrated probabilities.

# Calculate False Positive Rate (FPR) and True Positive Rate (TPR) for class 2
fpr, tpr, _ = roc_curve(y_test == 2, y_calib_prob[:, 2])

# Plot the ROC curve
plt.plot(fpr, tpr, label="High Risk ROC")  # Line representing ROC for High Risk
plt.xlabel("False Positive Rate")          # X-axis label
plt.ylabel("True Positive Rate")           # Y-axis label
plt.legend()                               # Show legend
plt.show()                                 # Display the plot

"""Explanation:

This code plots the ROC curve for the "High Risk" class.
It shows the trade-off between True Positive Rate and False Positive Rate,
helping evaluate the model's ability to identify this class correctly.

#Analyze model prediction confidence
"""

# We calculate the confidence of each prediction as the highest predicted probability.

# Get the maximum predicted probability for each sample
confidence = y_calib_prob.max(axis=1)

# Summarize confidence values using descriptive statistics
pd.Series(confidence).describe()

"""Explanation:

This code finds the confidence of each prediction (the highest probability predicted by the model).
The describe() function gives statistics like mean, min, max, and quartiles, showing how confident
the model is overall in its predictions.

#CLINICAL RULE
High Risk:
- Severe chest pain
- High cholesterol
- Abnormal ECG

Medium Risk:
- Age > 55 with risk factors

Low Risk:
- Normal vitals and no angina

#ETHICS
- Bias in dataset (gender, age)
- Risk of false negatives
- Model is decision-support only
- Privacy of medical data

#FINAL USER-INPUT MEDICAL TRIAGE CHECKER
"""

# ============================================================
# FINAL INNOVATION CELL: Interactive Medical Triage Prediction
# ============================================================
# Purpose: Take patient input and output risk level, score,
# color-coded triage, confidence, and medical action.
# ============================================================


# ---------- Risk Score & Color Coding ----------
def risk_score_and_color(prob):
    # Convert probability (0–1) into percentage score
    score = int(prob * 100)

    # Assign clinical triage color based on thresholds
    if score <= 30:
        return score, "Green (Low Risk)"
    elif score <= 60:
        return score, "Yellow (Moderate Risk)"
    else:
        return score, "Red (High Risk)"


# ---------- Select Final Trained Model ----------
# Prefer calibrated model if available
final_model = calibrated_model if 'calibrated_model' in globals() else calibrated_rf


# ---------- Interactive Risk Prediction ----------
def predict_patient_risk():

    # Display input header
    print("\n Enter Patient Clinical Information")
    print("-----------------------------------")

    # Collect key clinical features
    age = int(input("Enter age: "))
    sex = input("Enter sex (Male/Female): ")
    cp = int(input("Enter chest pain type (0–3): "))
    chol = float(input("Enter cholesterol level: "))
    thalach = float(input("Enter max heart rate achieved: "))

    # Create a single-row dataframe for prediction
    patient = pd.DataFrame([{
        "age": age,
        "sex": sex,
        "cp": cp,
        "chol": chol,
        "thalach": thalach
    }])

    # Add missing features using training data statistics
    for col in X.columns:
        if col not in patient.columns:
            if col in numeric_cols:
                patient[col] = X[col].median()   # Numeric → median
            else:
                patient[col] = X[col].mode()[0] # Categorical → mode

    # Ensure same feature order as training data
    patient = patient[X.columns]

    # Apply preprocessing pipeline
    patient_processed = preprocessor.transform(patient)

    # Predict class probabilities
    probs = final_model.predict_proba(patient_processed)[0]

    # Select predicted class and confidence
    risk_class = int(np.argmax(probs))
    confidence = float(np.max(probs))

    # Generate risk score and color from high-risk probability
    high_risk_prob = probs[2]
    risk_score, risk_color = risk_score_and_color(high_risk_prob)

    # Display prediction results
    print("\n Triage Prediction Result")
    print("---------------------------")

    # Map numeric labels to clinical terms
    risk_map = {0: "Low Risk", 1: "Moderate Risk", 2: "High Risk"}

    print(f"Predicted Risk Level : {risk_map[risk_class]}")
    print(f"Risk Score           : {risk_score}/100")
    print(f"Triage Color Code    : {risk_color}")
    print(f"Model Confidence     : {confidence:.2f}")
    print(f"Class Probabilities  : {probs}")

    # Recommend clinical action based on risk level
    print("\n Recommended Action")
    if risk_class == 2:
        print(" Immediate medical attention required")
    elif risk_class == 1:
        print(" Close monitoring and follow-up advised")
    else:
        print(" Routine check-up is sufficient")

    # Return results for further use if needed
    return {
        "risk_class": risk_map[risk_class],
        "risk_score": risk_score,
        "color": risk_color,
        "confidence": confidence,
        "probabilities": probs
    }


# ---------- Run the Prediction System ----------
predict_patient_risk()

"""Explanation:

This code allows real-time input of patient clinical features and predicts the risk level
using the trained and calibrated Random Forest model. It also provides a confidence score
and suggested medical action based on predicted risk.

"""